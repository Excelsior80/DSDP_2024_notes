{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecff1ec",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Suppose we want to fit a line of best fit through the following points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6bcb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate random x and y values\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)  # Setting seed for reproducibility\n",
    "x = np.random.rand(15)  # Generate 5 random x values between 0 and 1\n",
    "y = 2*x + 1 + np.random.rand(15)  # Generate 5 random y values between 0 and 1 and add to the line y = 2x + 1.\n",
    "# Note that because we know how y has been generated we will expect the coefficients of the line of \n",
    "# best fit to be approximately 2 and 1.\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y, color='b', marker='o')\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('X Values')\n",
    "plt.ylabel('Y Values')\n",
    "plt.title('Scatter Plot of Random Data')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf7ead",
   "metadata": {},
   "source": [
    "The line of best fit will be in the form $y = \\beta_1 x + \\beta_0$.\n",
    "\n",
    "We will use gradient descent to find the best values of $\\beta_1$ and $\\beta_0$ that MINIMISES the residuals.\n",
    "\n",
    "Residuals are the vertical differences between the data points and the line - they represent the errors that would be made if predicting the y-coordinate using the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823cb402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the difference between a predicted value of y (using the current guess for the equation of the line)\n",
    "# and the actual y coordinate for each point.\n",
    "# We would like to minimise the total of all these differences, but some will be positive and some negative.\n",
    "# One solution to this is to square the differences before totalling.\n",
    "# This gives rise to the Ordinary Least Squares (OLS) method, though there are other ways of tackling this problem.\n",
    "\n",
    "def squared_residuals(y_pred, y_actual):\n",
    "    # The Loss Function\n",
    "    return np.sum((y_pred - y_actual)**2)\n",
    "\n",
    "# As we try different values for beta_1 and beta_0 we will calculate the predicted y coordinate\n",
    "# and then use the gradient of the total of the squared residuals to adjust beta_1 and beta_0 until it's minimised.\n",
    "\n",
    "def calculate_y_pred(x, b1, b0):\n",
    "    return b1*x + b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4841517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in the first gradient descent example we'll initialise the learning rate and number of iterations\n",
    "# However, in this example we've set beta_1 and beta_0 to be zero initially (rather than random values)\n",
    "\n",
    "b1 = 0.0\n",
    "b0 = 0.0\n",
    "L = 0.001\n",
    "iterations = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f99d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.82460536468395"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll begin by calculating the total of the squared residuals \n",
    "# for the initial values of b1 and b0:\n",
    "y_pred = calculate_y_pred(x, b1, b0)\n",
    "\n",
    "loss = squared_residuals(y_pred, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d660f",
   "metadata": {},
   "source": [
    "We will now use this to adjust b1 and b0 using the gradient of the loss function (which in this case is the squared residuals). To do this we will need to partially differentiate with respect to $\\beta_1$ and $\\beta_0$ seperately. And notice we will also need to use Chain Rule.\n",
    "\n",
    "$SR = (y_{pred} - y_{actual})^2$\n",
    "\n",
    "Substituting $y = \\beta_1 x + \\beta_0$ we get:\n",
    "\n",
    "$SR = (\\beta_1 x + \\beta_0 - y_{actual})^2$\n",
    "\n",
    "We can now differentiate SR partially with respect to $\\beta_1$ and then $\\beta_0$ using Chain rule:\n",
    "\n",
    "$\\frac{\\partial SR}{\\partial \\beta_1} = 2(\\beta_1 x +\\beta_0 - y_{actual})x$\n",
    "\n",
    "and\n",
    "\n",
    "$\\frac{\\partial SR}{\\partial \\beta_0} = 2(\\beta_1 x +\\beta_0 - y_{actual})$\n",
    "\n",
    "Now that we no longer need to see where the $\\beta$'s are to differentiate we can simplify to:\n",
    "\n",
    "$\\frac{\\partial SR}{\\partial \\beta_1} = 2(y_{pred} - y_{actual})x$\n",
    "\n",
    "and\n",
    "\n",
    "$\\frac{\\partial SR}{\\partial \\beta_0} = 2(y_{pred} - y_{actual})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5b3ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After one iteration the coefficients have changed from zero to: 0.053336176478438545 and 0.08295313979480397\n"
     ]
    }
   ],
   "source": [
    "# Now adjust b1 and b0:\n",
    "\n",
    "b1 -= L*np.sum(2*(y_pred - y)*x)\n",
    "\n",
    "b0 -= L*np.sum(2*(y_pred - y))\n",
    "\n",
    "print('After one iteration the coefficients have changed from zero to:',b1,'and',b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88c54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 2.156495686868057x + 1.4650310370766957\n"
     ]
    }
   ],
   "source": [
    "# The more iterations (repeats) of this process we do the closer we will get to the best equation for the line of best fit\n",
    "# Remember that because we generated y values using the equation y = 2x + 1 with some random noise \n",
    "# we expect to get and answer close to this\n",
    "\n",
    "for count in range(iterations):\n",
    "    y_pred = calculate_y_pred(x, b1, b0)\n",
    "    slope = squared_residuals(y_pred, y)\n",
    "    b1 -= L*np.sum(2*(y_pred - y)*x)\n",
    "    b0 -= L*np.sum(2*(y_pred - y))\n",
    "    \n",
    "# and at the end we can see the equation we have reached\n",
    "print(f'y = {b1}x + {b0}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
